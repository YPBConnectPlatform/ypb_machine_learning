{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "import os, sys, time, csv\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ipywidgets as ipyw\n",
    "import inspect\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process image stacks into feature vector matrices using pre-trained model (for LSTM input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports.\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models.resnet\n",
    "\n",
    "# Visualization imports\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part A: Load in model pre-trained on individual images. Note that pre-training may not even be necessary, as I was only\n",
    "# changing the weights on the last (new) fully connected layer, and that layer will likely be gotten rid of. \n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "del model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble image pathnames and understand class breakdown in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9123 training stacks, 1141 validation stacks and 1141 test stacks.\n",
      "For the training stacks, 50.1% (4574) are positive and 49.9% (4549) are negative.\n",
      "For the validation stacks, 50.1% (572) are positive and 49.9% (569) are negative.\n",
      "For the test stacks, 50.1% (572) are positive and 49.9% (569) are negative.\n"
     ]
    }
   ],
   "source": [
    "# Note down train, validation and test directories.\n",
    "topDir = 'Split_Data'\n",
    "\n",
    "train_dir = topDir + \"/train\"\n",
    "valid_dir = topDir + \"/valid\"\n",
    "test_dir = topDir + \"/test\"\n",
    "\n",
    "# Check quantities of train, validation and test images\n",
    "train_images = np.array(glob(train_dir + \"/*/*\"))\n",
    "valid_images = np.array(glob(valid_dir + \"/*/*\"))\n",
    "test_images = np.array(glob(test_dir + \"/*/*\"))\n",
    "\n",
    "# Check relative percentages of image types\n",
    "train_images_absent = np.array(glob(train_dir + \"/Absent/*\"))\n",
    "train_images_present = np.array(glob(train_dir + \"/Present/*\"))\n",
    "\n",
    "valid_images_absent = np.array(glob(valid_dir + \"/Absent/*\"))\n",
    "valid_images_present = np.array(glob(valid_dir + \"/Present/*\"))\n",
    "\n",
    "test_images_absent = np.array(glob(test_dir + \"/Absent/*\"))\n",
    "test_images_present = np.array(glob(test_dir + \"/Present/*\"))\n",
    "\n",
    "num_train_images = len(train_images)\n",
    "num_valid_images = len(valid_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "print(\"There are {} training stacks, {} validation stacks and {} test stacks.\".format(len(train_images),len(valid_images),len(test_images)))\n",
    "print(\"For the training stacks, {pos:=.1f}% ({pos2}) are positive and {neg:=.1f}% ({neg2}) are negative.\".format(pos=len(train_images_present)/len(train_images)*100, pos2=len(train_images_present),neg=len(train_images_absent)/len(train_images)*100, neg2=len(train_images_absent)))\n",
    "print(\"For the validation stacks, {pos:=.1f}% ({pos2}) are positive and {neg:=.1f}% ({neg2}) are negative.\".format(pos=len(valid_images_present)/len(valid_images)*100, pos2=len(valid_images_present),neg=len(valid_images_absent)/len(valid_images)*100, neg2=len(valid_images_absent)))\n",
    "print(\"For the test stacks, {pos:=.1f}% ({pos2}) are positive and {neg:=.1f}% ({neg2}) are negative.\".format(pos=len(test_images_present)/len(test_images)*100, pos2=len(test_images_present),neg=len(test_images_absent)/len(test_images)*100, neg2=len(test_images_absent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU support\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPUs found.\n"
     ]
    }
   ],
   "source": [
    "# Check to see how many GPUs are available.\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"{} GPUs found.\".format(num_devices))\n",
    "else:\n",
    "    num_devices = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot sample image stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following https://github.com/mohakpatel/ImageSliceViewer3D, with modifications for plotting multiple stacks of color images. \n",
    "'''class ImageSliceViewer:\n",
    "    \"\"\" \n",
    "    ImageSliceViewer is for viewing image stacks in jupyter or\n",
    "    ipython notebooks. \n",
    "    \n",
    "    User can interactively change the slice plane selection for the image.\n",
    "\n",
    "    Argumentss:\n",
    "    Volume = stack of color input images [num images, height, width, RGB]\n",
    "    figsize = default(8,8), to set the size of the figure\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, volume, figsize=(10,10)):\n",
    "        self.volume = volume\n",
    "        self.figsize = figsize\n",
    "        self.v = [np.min(volume), np.max(volume)]\n",
    "        \n",
    "        # Call to select slice plane\n",
    "        ipyw.interact(self.view_selection, view=ipyw.RadioButtons(\n",
    "            options=['x-y'], value='x-y', \n",
    "            description='Slice plane selection:', disabled=False,\n",
    "            style={'description_width': 'initial'}))\n",
    "    \n",
    "    def view_selection(self, view):\n",
    "        # View the volume\n",
    "        maxZ = self.volume.shape[0] - 1\n",
    "        \n",
    "        # Call to view a slice within the selected slice plane\n",
    "        ipyw.interact(self.plot_slice, \n",
    "            z=ipyw.IntSlider(min=0, max=maxZ, step=1, continuous_update=False, \n",
    "            description='Image Slice:'))\n",
    "        \n",
    "    def plot_slice(self, z):\n",
    "        # Plot slice for the given plane and slice\n",
    "        fig=plt.figure(figsize=self.figsize)\n",
    "        plt.imshow(self.volume[z,:,:,:], \n",
    "            vmin=self.v[0], vmax=self.v[1])\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def show_an_image_stack(dataset):\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    axes = []\n",
    "    # Grab a stack at random and build a volume.\n",
    "    directories = os.listdir(dataset)\n",
    "    directory = directories[np.random.randint(0,len(directories)-1)]\n",
    "    files = os.listdir(os.path.join(dataset,directory))\n",
    "    print(\"Plotting image stack {}\".format(os.path.join(dataset,directory)))\n",
    "    files.sort()\n",
    "    imgArr = []\n",
    "    for file in files:\n",
    "        file = os.path.join(dataset,directory, file)\n",
    "        imgArr.append(np.array(Image.open(file)))\n",
    "    imgArr = np.array(imgArr)\n",
    "    # Plot the volume within a subplot\n",
    "    ImageSliceViewer(imgArr)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_an_image_stack('Split_Data/train/Present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports for torch and DALI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MM_DNN(nn.Module):\n",
    "    def __init__(self, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(4*1024,256)\n",
    "        self.fc2 = nn.Linear(256,64)\n",
    "        self.fc3 = nn.Linear(64,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        x = x.view(-1)\n",
    "        out = self.dropout(self.relu(self.fc(x)))\n",
    "        out = self.dropout(self.relu(self.fc2(out)))\n",
    "        out = self.fc3(out)\n",
    "        # return the final output\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PyTorch data loader here, way easier. \n",
    "def load_pickle(path):\n",
    "    with open(path,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "train_data = datasets.DatasetFolder(train_dir,load_pickle,'.pickle')\n",
    "validation_data = datasets.DatasetFolder(valid_dir,load_pickle,'.pickle')\n",
    "test_data = datasets.DatasetFolder(test_dir,load_pickle,'.pickle')\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 1, shuffle=True, drop_last=True, num_workers=num_devices*4, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validation_data, batch_size = 1, num_workers=num_devices*4, pin_memory=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 1, num_workers=num_devices*4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, epochs=10, batch_size=2, lr=0.001, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: DNN network\n",
    "        train_loader: PyTorch dataloader containing data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        lr: learning rate\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    valid_batch = 1\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if(use_cuda):\n",
    "        net.cuda()\n",
    "    all_valid_losses = [.6]\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            inputs = batch_data[0].view(12,batch_size,1024)\n",
    "            inputs = inputs[0:4]\n",
    "            targets = batch_data[1]\n",
    "            if(use_cuda):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.view(batch_size,2), targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            # loss stats\n",
    "            if batch_idx % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                correct = 0.\n",
    "                total = 0.\n",
    "                for val_batch_idx, batch_data in enumerate(valid_loader):                    \n",
    "                    inputs, targets = batch_data[0].view(12,valid_batch,1024), batch_data[1]\n",
    "                    inputs = inputs[0:4]\n",
    "                    \n",
    "                    if(use_cuda):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.view(valid_batch,2), targets)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    pred = output.max(0,keepdim=True)[1]\n",
    "                    # compare predictions to true label\n",
    "                    correct += np.sum(np.squeeze(pred.eq(targets.data.view_as(pred))).cpu().numpy())\n",
    "                    total += valid_batch\n",
    "                \n",
    "                net.train() # reset to train mode after iteration through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(batch_idx),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "                      \"Val Accuracy: {:.1f}%\".format(correct/total*100))\n",
    "                \n",
    "                if np.mean(val_losses) < np.min(all_valid_losses):\n",
    "                    print(\"Validation loss dropped, saving model.\")\n",
    "                    torch.save(net,'DNNModel_' + saveName + '.pt')\n",
    "                    all_valid_losses = np.append(all_valid_losses,np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MM_DNN(\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (fc): Linear(in_features=4096, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "\n",
    "net = MM_DNN()\n",
    "#net = MM_LSTM(32,1)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 0... Loss: 0.8331... Val Loss: 0.6937... Val Accuracy: 49.9%\n",
      "Epoch: 1/50... Step: 6000... Loss: 0.6952... Val Loss: 0.3042... Val Accuracy: 85.5%\n",
      "Validation loss dropped, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/derm-ai/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MM_DNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/50... Step: 0... Loss: 0.1976... Val Loss: 0.2358... Val Accuracy: 89.6%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 2/50... Step: 6000... Loss: 0.4668... Val Loss: 0.2029... Val Accuracy: 91.3%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 3/50... Step: 0... Loss: 0.0383... Val Loss: 0.1827... Val Accuracy: 92.1%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 3/50... Step: 6000... Loss: 0.0023... Val Loss: 0.1620... Val Accuracy: 93.8%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 4/50... Step: 0... Loss: 1.6973... Val Loss: 0.1583... Val Accuracy: 93.1%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 4/50... Step: 6000... Loss: 0.0063... Val Loss: 0.1310... Val Accuracy: 95.0%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 5/50... Step: 0... Loss: 0.0020... Val Loss: 0.1373... Val Accuracy: 94.7%\n",
      "Epoch: 5/50... Step: 6000... Loss: 0.0264... Val Loss: 0.1556... Val Accuracy: 93.9%\n",
      "Epoch: 6/50... Step: 0... Loss: 0.0923... Val Loss: 0.1483... Val Accuracy: 93.9%\n",
      "Epoch: 6/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1936... Val Accuracy: 92.4%\n",
      "Epoch: 7/50... Step: 0... Loss: 0.0000... Val Loss: 0.1523... Val Accuracy: 94.3%\n",
      "Epoch: 7/50... Step: 6000... Loss: 0.0001... Val Loss: 0.1093... Val Accuracy: 96.1%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 8/50... Step: 0... Loss: 0.0002... Val Loss: 0.1644... Val Accuracy: 94.2%\n",
      "Epoch: 8/50... Step: 6000... Loss: 0.0061... Val Loss: 0.1248... Val Accuracy: 95.4%\n",
      "Epoch: 9/50... Step: 0... Loss: 0.0012... Val Loss: 0.1330... Val Accuracy: 95.4%\n",
      "Epoch: 9/50... Step: 6000... Loss: 0.0006... Val Loss: 0.1178... Val Accuracy: 94.9%\n",
      "Epoch: 10/50... Step: 0... Loss: 0.1259... Val Loss: 0.1041... Val Accuracy: 95.7%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 10/50... Step: 6000... Loss: 0.0002... Val Loss: 0.1246... Val Accuracy: 95.7%\n",
      "Epoch: 11/50... Step: 0... Loss: 0.0024... Val Loss: 0.1065... Val Accuracy: 95.7%\n",
      "Epoch: 11/50... Step: 6000... Loss: 0.0007... Val Loss: 0.1023... Val Accuracy: 96.4%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 12/50... Step: 0... Loss: 0.0002... Val Loss: 0.1173... Val Accuracy: 95.6%\n",
      "Epoch: 12/50... Step: 6000... Loss: 0.0728... Val Loss: 0.0983... Val Accuracy: 96.1%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 13/50... Step: 0... Loss: 0.0001... Val Loss: 0.0963... Val Accuracy: 96.3%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 13/50... Step: 6000... Loss: 0.0011... Val Loss: 0.0910... Val Accuracy: 96.7%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 14/50... Step: 0... Loss: 0.0133... Val Loss: 0.0937... Val Accuracy: 96.8%\n",
      "Epoch: 14/50... Step: 6000... Loss: 0.0008... Val Loss: 0.0965... Val Accuracy: 96.6%\n",
      "Epoch: 15/50... Step: 0... Loss: 0.0073... Val Loss: 0.1107... Val Accuracy: 96.2%\n",
      "Epoch: 15/50... Step: 6000... Loss: 0.0127... Val Loss: 0.1107... Val Accuracy: 96.1%\n",
      "Epoch: 16/50... Step: 0... Loss: 0.0000... Val Loss: 0.0937... Val Accuracy: 96.4%\n",
      "Epoch: 16/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1005... Val Accuracy: 96.4%\n",
      "Epoch: 17/50... Step: 0... Loss: 0.0556... Val Loss: 0.1255... Val Accuracy: 95.8%\n",
      "Epoch: 17/50... Step: 6000... Loss: 0.0001... Val Loss: 0.1012... Val Accuracy: 96.7%\n",
      "Epoch: 18/50... Step: 0... Loss: 0.0048... Val Loss: 0.0921... Val Accuracy: 96.8%\n",
      "Epoch: 18/50... Step: 6000... Loss: 0.0002... Val Loss: 0.0936... Val Accuracy: 96.5%\n",
      "Epoch: 19/50... Step: 0... Loss: 0.0001... Val Loss: 0.1143... Val Accuracy: 96.4%\n",
      "Epoch: 19/50... Step: 6000... Loss: 0.0001... Val Loss: 0.0940... Val Accuracy: 96.8%\n",
      "Epoch: 20/50... Step: 0... Loss: 0.0001... Val Loss: 0.0911... Val Accuracy: 96.6%\n",
      "Epoch: 20/50... Step: 6000... Loss: 0.0001... Val Loss: 0.1105... Val Accuracy: 96.1%\n",
      "Epoch: 21/50... Step: 0... Loss: 0.0020... Val Loss: 0.1080... Val Accuracy: 96.3%\n",
      "Epoch: 21/50... Step: 6000... Loss: 0.0000... Val Loss: 0.0941... Val Accuracy: 96.8%\n",
      "Epoch: 22/50... Step: 0... Loss: 0.0958... Val Loss: 0.1008... Val Accuracy: 96.4%\n",
      "Epoch: 22/50... Step: 6000... Loss: 0.0003... Val Loss: 0.1022... Val Accuracy: 97.3%\n",
      "Epoch: 23/50... Step: 0... Loss: 0.0012... Val Loss: 0.0905... Val Accuracy: 96.7%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 23/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1055... Val Accuracy: 96.7%\n",
      "Epoch: 24/50... Step: 0... Loss: 0.0005... Val Loss: 0.1057... Val Accuracy: 97.0%\n",
      "Epoch: 24/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1067... Val Accuracy: 96.6%\n",
      "Epoch: 25/50... Step: 0... Loss: 0.0000... Val Loss: 0.1485... Val Accuracy: 95.4%\n",
      "Epoch: 25/50... Step: 6000... Loss: 0.0002... Val Loss: 0.0954... Val Accuracy: 97.0%\n",
      "Epoch: 26/50... Step: 0... Loss: 0.0003... Val Loss: 0.1223... Val Accuracy: 96.2%\n",
      "Epoch: 26/50... Step: 6000... Loss: 0.0004... Val Loss: 0.0929... Val Accuracy: 96.9%\n",
      "Epoch: 27/50... Step: 0... Loss: 0.0009... Val Loss: 0.1377... Val Accuracy: 96.0%\n",
      "Epoch: 27/50... Step: 6000... Loss: 0.0001... Val Loss: 0.1097... Val Accuracy: 96.9%\n",
      "Epoch: 28/50... Step: 0... Loss: 0.0000... Val Loss: 0.1427... Val Accuracy: 95.4%\n",
      "Epoch: 28/50... Step: 6000... Loss: 0.0000... Val Loss: 0.0932... Val Accuracy: 96.5%\n",
      "Epoch: 29/50... Step: 0... Loss: 0.0001... Val Loss: 0.1241... Val Accuracy: 96.4%\n",
      "Epoch: 29/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1123... Val Accuracy: 97.0%\n",
      "Epoch: 30/50... Step: 0... Loss: 0.0019... Val Loss: 0.1092... Val Accuracy: 96.6%\n",
      "Epoch: 30/50... Step: 6000... Loss: 0.0089... Val Loss: 0.0854... Val Accuracy: 97.0%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 31/50... Step: 0... Loss: 0.0012... Val Loss: 0.1054... Val Accuracy: 96.9%\n",
      "Epoch: 31/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1208... Val Accuracy: 97.0%\n",
      "Epoch: 32/50... Step: 0... Loss: 0.0000... Val Loss: 0.1081... Val Accuracy: 96.1%\n",
      "Epoch: 32/50... Step: 6000... Loss: 0.0000... Val Loss: 0.0997... Val Accuracy: 96.2%\n",
      "Epoch: 33/50... Step: 0... Loss: 0.0000... Val Loss: 0.1098... Val Accuracy: 96.7%\n",
      "Epoch: 33/50... Step: 6000... Loss: 0.0017... Val Loss: 0.1359... Val Accuracy: 97.2%\n",
      "Epoch: 34/50... Step: 0... Loss: 0.0000... Val Loss: 0.1129... Val Accuracy: 96.6%\n",
      "Epoch: 34/50... Step: 6000... Loss: 0.0007... Val Loss: 0.0973... Val Accuracy: 96.9%\n",
      "Epoch: 35/50... Step: 0... Loss: 0.0027... Val Loss: 0.1325... Val Accuracy: 95.8%\n",
      "Epoch: 35/50... Step: 6000... Loss: 0.0007... Val Loss: 0.1346... Val Accuracy: 96.3%\n",
      "Epoch: 36/50... Step: 0... Loss: 0.0000... Val Loss: 0.1444... Val Accuracy: 96.7%\n",
      "Epoch: 36/50... Step: 6000... Loss: 0.0127... Val Loss: 0.1316... Val Accuracy: 96.1%\n",
      "Epoch: 37/50... Step: 0... Loss: 0.0484... Val Loss: 0.1001... Val Accuracy: 97.3%\n",
      "Epoch: 37/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1239... Val Accuracy: 96.1%\n",
      "Epoch: 38/50... Step: 0... Loss: 0.0000... Val Loss: 0.1229... Val Accuracy: 96.2%\n",
      "Epoch: 38/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1637... Val Accuracy: 96.2%\n",
      "Epoch: 39/50... Step: 0... Loss: 0.0000... Val Loss: 0.1069... Val Accuracy: 96.8%\n",
      "Epoch: 39/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1072... Val Accuracy: 96.8%\n",
      "Epoch: 40/50... Step: 0... Loss: 0.0000... Val Loss: 0.1477... Val Accuracy: 96.8%\n",
      "Epoch: 40/50... Step: 6000... Loss: 0.0002... Val Loss: 0.1042... Val Accuracy: 96.5%\n",
      "Epoch: 41/50... Step: 0... Loss: 0.0000... Val Loss: 0.1188... Val Accuracy: 96.2%\n",
      "Epoch: 41/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1732... Val Accuracy: 96.4%\n",
      "Epoch: 42/50... Step: 0... Loss: 0.0000... Val Loss: 0.1298... Val Accuracy: 97.1%\n",
      "Epoch: 42/50... Step: 6000... Loss: 0.1013... Val Loss: 0.1109... Val Accuracy: 97.0%\n",
      "Epoch: 43/50... Step: 0... Loss: 0.0000... Val Loss: 0.1057... Val Accuracy: 96.7%\n",
      "Epoch: 43/50... Step: 6000... Loss: 0.1734... Val Loss: 0.1340... Val Accuracy: 96.5%\n",
      "Epoch: 44/50... Step: 0... Loss: 0.0016... Val Loss: 0.1256... Val Accuracy: 96.7%\n",
      "Epoch: 44/50... Step: 6000... Loss: 0.0000... Val Loss: 0.0996... Val Accuracy: 97.0%\n",
      "Epoch: 45/50... Step: 0... Loss: 0.0000... Val Loss: 0.1038... Val Accuracy: 96.3%\n",
      "Epoch: 45/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1026... Val Accuracy: 96.7%\n",
      "Epoch: 46/50... Step: 0... Loss: 0.0046... Val Loss: 0.1094... Val Accuracy: 96.8%\n",
      "Epoch: 46/50... Step: 6000... Loss: 0.0093... Val Loss: 0.1212... Val Accuracy: 96.1%\n",
      "Epoch: 47/50... Step: 0... Loss: 0.0017... Val Loss: 0.1228... Val Accuracy: 97.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1162... Val Accuracy: 96.2%\n",
      "Epoch: 48/50... Step: 0... Loss: 0.0014... Val Loss: 0.1652... Val Accuracy: 96.6%\n",
      "Epoch: 48/50... Step: 6000... Loss: 0.0037... Val Loss: 0.1277... Val Accuracy: 96.8%\n",
      "Epoch: 49/50... Step: 0... Loss: 0.0000... Val Loss: 0.1176... Val Accuracy: 96.8%\n",
      "Epoch: 49/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1269... Val Accuracy: 96.7%\n",
      "Epoch: 50/50... Step: 0... Loss: 0.0001... Val Loss: 0.0958... Val Accuracy: 96.8%\n",
      "Epoch: 50/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1112... Val Accuracy: 97.2%\n"
     ]
    }
   ],
   "source": [
    "train(net,train_loader,30,1,print_every=6000,lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('DNNModel_' + saveName + '.pt')\n",
    "stateDictName ='DNN_StateDict_' + saveName + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), stateDictName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(stateDictName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('DNN_StateDict_S3_Iter_1.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip up state dict, get over to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below lines are only needed if allowing SageMaker to perform inference on the model.\n",
    "# I can walk you through that when you get there.\n",
    "\n",
    "#tarName = 'DNNModel_' + saveName + '.tar.gz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.system('tar -cvzf ' + tarName + ' ' + stateDictName)\n",
    "#os.system('aws s3 cp ' + tarName + ' s3://ypb-ml-images/' + tarName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_derm-ai)",
   "language": "python",
   "name": "conda_derm-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
