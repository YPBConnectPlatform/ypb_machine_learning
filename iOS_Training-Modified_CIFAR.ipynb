{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "import os, sys, time, csv\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ipywidgets as ipyw\n",
    "import inspect\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process image stacks into feature vector matrices using pre-trained model (for LSTM input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports.\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models.resnet\n",
    "\n",
    "# Visualization imports\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Load in model pre-trained on individual images. Note that pre-training may not even be necessary, as I was only\n",
    "# changing the weights on the last (new) fully connected layer, and that layer will likely be gotten rid of. \n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "del model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble image pathnames and understand class breakdown in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9123 training stacks, 1141 validation stacks and 1141 test stacks.\n",
      "For the training stacks, 50.1% (4574) are positive and 49.9% (4549) are negative.\n",
      "For the validation stacks, 50.1% (572) are positive and 49.9% (569) are negative.\n",
      "For the test stacks, 50.1% (572) are positive and 49.9% (569) are negative.\n"
     ]
    }
   ],
   "source": [
    "# Note down train, validation and test directories.\n",
    "topDir = 'Split_Data2'\n",
    "\n",
    "train_dir = topDir + \"/train\"\n",
    "valid_dir = topDir + \"/valid\"\n",
    "test_dir = topDir + \"/test\"\n",
    "\n",
    "# Check quantities of train, validation and test images\n",
    "train_images = np.array(glob(train_dir + \"/*/*\"))\n",
    "valid_images = np.array(glob(valid_dir + \"/*/*\"))\n",
    "test_images = np.array(glob(test_dir + \"/*/*\"))\n",
    "\n",
    "# Check relative percentages of image types\n",
    "train_images_absent = np.array(glob(train_dir + \"/Absent/*\"))\n",
    "train_images_present = np.array(glob(train_dir + \"/Present/*\"))\n",
    "\n",
    "valid_images_absent = np.array(glob(valid_dir + \"/Absent/*\"))\n",
    "valid_images_present = np.array(glob(valid_dir + \"/Present/*\"))\n",
    "\n",
    "test_images_absent = np.array(glob(test_dir + \"/Absent/*\"))\n",
    "test_images_present = np.array(glob(test_dir + \"/Present/*\"))\n",
    "\n",
    "num_train_images = len(train_images)\n",
    "num_valid_images = len(valid_images)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "print(\"There are {} training stacks, {} validation stacks and {} test stacks.\".format(len(train_images),len(valid_images),len(test_images)))\n",
    "print(\"For the training stacks, {pos:=.1f}% ({pos2}) are positive and {neg:=.1f}% ({neg2}) are negative.\".format(pos=len(train_images_present)/len(train_images)*100, pos2=len(train_images_present),neg=len(train_images_absent)/len(train_images)*100, neg2=len(train_images_absent)))\n",
    "print(\"For the validation stacks, {pos:=.1f}% ({pos2}) are positive and {neg:=.1f}% ({neg2}) are negative.\".format(pos=len(valid_images_present)/len(valid_images)*100, pos2=len(valid_images_present),neg=len(valid_images_absent)/len(valid_images)*100, neg2=len(valid_images_absent)))\n",
    "print(\"For the test stacks, {pos:=.1f}% ({pos2}) are positive and {neg:=.1f}% ({neg2}) are negative.\".format(pos=len(test_images_present)/len(test_images)*100, pos2=len(test_images_present),neg=len(test_images_absent)/len(test_images)*100, neg2=len(test_images_absent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU support\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPUs found.\n"
     ]
    }
   ],
   "source": [
    "# Check to see how many GPUs are available.\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"{} GPUs found.\".format(num_devices))\n",
    "else:\n",
    "    num_devices = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports for torch and DALI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(224, 224, 3)\n",
    "#         self.conv2 = nn.Conv2d(224, 224, 64) #2 layers\n",
    "#         # http://web.media.mit.edu/~pratiks/combined-classification/convolutional-neural-network-for-combined-classification-of-fluorescent-biomarkers-and-expert-annotations-using-white-light-images.pdf\n",
    "#         # https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MM_DNN(nn.Module):\n",
    "    def __init__(self, drop_prob=0.2, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(4*1024,256)\n",
    "        self.fc2 = nn.Linear(256,64)\n",
    "        self.fc3 = nn.Linear(64,2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        x = x.view(-1)\n",
    "        out = self.dropout(self.relu(self.fc(x)))\n",
    "        out = self.dropout(self.relu(self.fc2(out)))\n",
    "        out = self.fc3(out)\n",
    "        # return the final output\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PyTorch data loader here, way easier. \n",
    "def load_pickle(path):\n",
    "    with open(path,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "train_data = datasets.DatasetFolder(train_dir,load_pickle,'.pickle')\n",
    "validation_data = datasets.DatasetFolder(valid_dir,load_pickle,'.pickle')\n",
    "test_data = datasets.DatasetFolder(test_dir,load_pickle,'.pickle')\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 1, shuffle=True, drop_last=True, num_workers=num_devices*4, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validation_data, batch_size = 1, num_workers=num_devices*4, pin_memory=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = 1, num_workers=num_devices*4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 3, 32, 32])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = load_pickle('/home/ubuntu/src/YPB-AI/Split_Data2/valid/Absent/XS_Negative_Dark_14-Mar-2019_18-53-16.pickle')\n",
    "x.shape\n",
    "# image, label = iter(train_loader).next()\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()[0]\n",
    "    print(npimg.shape)\n",
    "    plt.imshow(np.transpose(npimg,(1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'Testing_CIFAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, epochs=10, batch_size=2, lr=0.001, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: DNN network\n",
    "        train_loader: PyTorch dataloader containing data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        lr: learning rate\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    valid_batch = 1\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if(use_cuda):\n",
    "        net.cuda()\n",
    "    all_valid_losses = [.6]\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            inputs = batch_data[0].view(12,batch_size,1024)\n",
    "            inputs = inputs[0:4]\n",
    "            targets = batch_data[1]\n",
    "            if(use_cuda):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.view(batch_size,2), targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            # loss stats\n",
    "            if batch_idx % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                correct = 0.\n",
    "                total = 0.\n",
    "                for val_batch_idx, batch_data in enumerate(valid_loader):                    \n",
    "                    inputs, targets = batch_data[0].view(12,valid_batch,1024), batch_data[1]\n",
    "                    inputs = inputs[0:4]\n",
    "                    \n",
    "                    if(use_cuda):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.view(valid_batch,2), targets)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    pred = output.max(0,keepdim=True)[1]\n",
    "                    # compare predictions to true label\n",
    "                    correct += np.sum(np.squeeze(pred.eq(targets.data.view_as(pred))).cpu().numpy())\n",
    "                    total += valid_batch\n",
    "                \n",
    "                net.train() # reset to train mode after iteration through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(batch_idx),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "                      \"Val Accuracy: {:.1f}%\".format(correct/total*100))\n",
    "                \n",
    "                if np.mean(val_losses) < np.min(all_valid_losses):\n",
    "                    print(\"Validation loss dropped, saving model.\")\n",
    "                    torch.save(net,'DNNModel_' + saveName + '.pt')\n",
    "                    all_valid_losses = np.append(all_valid_losses,np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net,train_loader,50,1,print_every=6000,lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html \n",
    "class CIFARNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFARNet1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = CIFARNet1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARNet1(\n",
      "  (conv1): Conv2d(12, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "\n",
    "# net = MM_DNN()\n",
    "net = CIFARNet1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 0... Loss: 0.0034... Val Loss: 0.1720... Val Accuracy: 96.1%\n",
      "Validation loss dropped, saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/derm-ai/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type CIFARNet1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1109... Val Accuracy: 96.3%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 2/50... Step: 0... Loss: 0.0031... Val Loss: 0.1221... Val Accuracy: 96.2%\n",
      "Epoch: 2/50... Step: 6000... Loss: 0.0321... Val Loss: 0.1320... Val Accuracy: 95.9%\n",
      "Epoch: 3/50... Step: 0... Loss: 0.1371... Val Loss: 0.1170... Val Accuracy: 95.4%\n",
      "Epoch: 3/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1556... Val Accuracy: 96.1%\n",
      "Epoch: 4/50... Step: 0... Loss: 0.0000... Val Loss: 0.1299... Val Accuracy: 95.7%\n",
      "Epoch: 4/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1748... Val Accuracy: 96.1%\n",
      "Epoch: 5/50... Step: 0... Loss: 0.0000... Val Loss: 0.1509... Val Accuracy: 96.0%\n",
      "Epoch: 5/50... Step: 6000... Loss: 0.0142... Val Loss: 0.1210... Val Accuracy: 95.3%\n",
      "Epoch: 6/50... Step: 0... Loss: 0.0092... Val Loss: 0.1608... Val Accuracy: 95.0%\n",
      "Epoch: 6/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1249... Val Accuracy: 96.4%\n",
      "Epoch: 7/50... Step: 0... Loss: 0.9611... Val Loss: 0.1396... Val Accuracy: 96.6%\n",
      "Epoch: 7/50... Step: 6000... Loss: 0.0972... Val Loss: 0.1385... Val Accuracy: 96.3%\n",
      "Epoch: 8/50... Step: 0... Loss: 3.1838... Val Loss: 0.1653... Val Accuracy: 96.6%\n",
      "Epoch: 8/50... Step: 6000... Loss: 0.0012... Val Loss: 0.1647... Val Accuracy: 96.1%\n",
      "Epoch: 9/50... Step: 0... Loss: 0.0069... Val Loss: 0.1652... Val Accuracy: 96.5%\n",
      "Epoch: 9/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1722... Val Accuracy: 94.0%\n",
      "Epoch: 10/50... Step: 0... Loss: 0.0032... Val Loss: 0.2045... Val Accuracy: 95.8%\n",
      "Epoch: 10/50... Step: 6000... Loss: 0.0035... Val Loss: 0.1541... Val Accuracy: 96.1%\n",
      "Epoch: 11/50... Step: 0... Loss: 0.0000... Val Loss: 0.1240... Val Accuracy: 96.6%\n",
      "Epoch: 11/50... Step: 6000... Loss: 0.0490... Val Loss: 0.1270... Val Accuracy: 96.5%\n",
      "Epoch: 12/50... Step: 0... Loss: 0.0002... Val Loss: 0.2070... Val Accuracy: 96.5%\n",
      "Epoch: 12/50... Step: 6000... Loss: 0.0003... Val Loss: 0.1313... Val Accuracy: 96.3%\n",
      "Epoch: 13/50... Step: 0... Loss: 0.0000... Val Loss: 0.1296... Val Accuracy: 96.7%\n",
      "Epoch: 13/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1566... Val Accuracy: 96.4%\n",
      "Epoch: 14/50... Step: 0... Loss: 0.0000... Val Loss: 0.2930... Val Accuracy: 94.3%\n",
      "Epoch: 14/50... Step: 6000... Loss: 0.0023... Val Loss: 0.1769... Val Accuracy: 95.9%\n",
      "Epoch: 15/50... Step: 0... Loss: 0.0004... Val Loss: 0.2468... Val Accuracy: 94.7%\n",
      "Epoch: 15/50... Step: 6000... Loss: 0.0002... Val Loss: 0.1593... Val Accuracy: 96.7%\n",
      "Epoch: 16/50... Step: 0... Loss: 0.0000... Val Loss: 0.1341... Val Accuracy: 96.4%\n",
      "Epoch: 16/50... Step: 6000... Loss: 0.8484... Val Loss: 0.1228... Val Accuracy: 96.7%\n",
      "Epoch: 17/50... Step: 0... Loss: 0.0123... Val Loss: 0.1203... Val Accuracy: 96.3%\n",
      "Epoch: 17/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1185... Val Accuracy: 96.4%\n",
      "Epoch: 18/50... Step: 0... Loss: 0.0000... Val Loss: 0.1072... Val Accuracy: 96.8%\n",
      "Validation loss dropped, saving model.\n",
      "Epoch: 18/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1152... Val Accuracy: 96.2%\n",
      "Epoch: 19/50... Step: 0... Loss: 0.0000... Val Loss: 0.1961... Val Accuracy: 94.7%\n",
      "Epoch: 19/50... Step: 6000... Loss: 0.0066... Val Loss: 0.1346... Val Accuracy: 96.4%\n",
      "Epoch: 20/50... Step: 0... Loss: 0.0002... Val Loss: 0.1617... Val Accuracy: 96.0%\n",
      "Epoch: 20/50... Step: 6000... Loss: 0.0001... Val Loss: 0.1300... Val Accuracy: 96.2%\n",
      "Epoch: 21/50... Step: 0... Loss: 0.0001... Val Loss: 0.1225... Val Accuracy: 96.8%\n",
      "Epoch: 21/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1077... Val Accuracy: 96.2%\n",
      "Epoch: 22/50... Step: 0... Loss: 0.0000... Val Loss: 0.3180... Val Accuracy: 86.0%\n",
      "Epoch: 22/50... Step: 6000... Loss: 0.0007... Val Loss: 0.1329... Val Accuracy: 96.7%\n",
      "Epoch: 23/50... Step: 0... Loss: 0.0000... Val Loss: 0.2379... Val Accuracy: 94.4%\n",
      "Epoch: 23/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2078... Val Accuracy: 95.6%\n",
      "Epoch: 24/50... Step: 0... Loss: 0.6631... Val Loss: 0.1703... Val Accuracy: 94.1%\n",
      "Epoch: 24/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1574... Val Accuracy: 95.5%\n",
      "Epoch: 25/50... Step: 0... Loss: 0.0180... Val Loss: 0.1254... Val Accuracy: 96.7%\n",
      "Epoch: 25/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1623... Val Accuracy: 96.8%\n",
      "Epoch: 26/50... Step: 0... Loss: 0.1459... Val Loss: 0.2286... Val Accuracy: 96.0%\n",
      "Epoch: 26/50... Step: 6000... Loss: 0.0150... Val Loss: 0.1268... Val Accuracy: 97.1%\n",
      "Epoch: 27/50... Step: 0... Loss: 0.0000... Val Loss: 0.1540... Val Accuracy: 96.6%\n",
      "Epoch: 27/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1427... Val Accuracy: 96.7%\n",
      "Epoch: 28/50... Step: 0... Loss: 0.0001... Val Loss: 0.1614... Val Accuracy: 96.8%\n",
      "Epoch: 28/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1473... Val Accuracy: 97.1%\n",
      "Epoch: 29/50... Step: 0... Loss: 0.0000... Val Loss: 0.1621... Val Accuracy: 95.0%\n",
      "Epoch: 29/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2623... Val Accuracy: 96.5%\n",
      "Epoch: 30/50... Step: 0... Loss: 0.0945... Val Loss: 0.2236... Val Accuracy: 95.7%\n",
      "Epoch: 30/50... Step: 6000... Loss: 0.0361... Val Loss: 0.1858... Val Accuracy: 96.0%\n",
      "Epoch: 31/50... Step: 0... Loss: 0.0000... Val Loss: 0.1331... Val Accuracy: 96.5%\n",
      "Epoch: 31/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1508... Val Accuracy: 96.7%\n",
      "Epoch: 32/50... Step: 0... Loss: 0.0000... Val Loss: 0.1475... Val Accuracy: 96.7%\n",
      "Epoch: 32/50... Step: 6000... Loss: 0.0067... Val Loss: 0.1315... Val Accuracy: 97.0%\n",
      "Epoch: 33/50... Step: 0... Loss: 0.1103... Val Loss: 0.1658... Val Accuracy: 96.5%\n",
      "Epoch: 33/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2017... Val Accuracy: 95.4%\n",
      "Epoch: 34/50... Step: 0... Loss: 0.0000... Val Loss: 0.1775... Val Accuracy: 96.7%\n",
      "Epoch: 34/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1108... Val Accuracy: 97.2%\n",
      "Epoch: 35/50... Step: 0... Loss: 0.0000... Val Loss: 0.1877... Val Accuracy: 97.0%\n",
      "Epoch: 35/50... Step: 6000... Loss: 0.0189... Val Loss: 0.1971... Val Accuracy: 96.6%\n",
      "Epoch: 36/50... Step: 0... Loss: 0.0000... Val Loss: 0.2724... Val Accuracy: 96.6%\n",
      "Epoch: 36/50... Step: 6000... Loss: 0.0383... Val Loss: 0.2786... Val Accuracy: 95.6%\n",
      "Epoch: 37/50... Step: 0... Loss: 1.6120... Val Loss: 0.2485... Val Accuracy: 95.4%\n",
      "Epoch: 37/50... Step: 6000... Loss: 2.6744... Val Loss: 0.2595... Val Accuracy: 96.1%\n",
      "Epoch: 38/50... Step: 0... Loss: 0.2341... Val Loss: 0.4166... Val Accuracy: 94.1%\n",
      "Epoch: 38/50... Step: 6000... Loss: 0.0060... Val Loss: 0.2354... Val Accuracy: 96.5%\n",
      "Epoch: 39/50... Step: 0... Loss: 0.0000... Val Loss: 0.1803... Val Accuracy: 96.0%\n",
      "Epoch: 39/50... Step: 6000... Loss: 0.0000... Val Loss: 0.3721... Val Accuracy: 94.9%\n",
      "Epoch: 40/50... Step: 0... Loss: 0.0000... Val Loss: 0.2143... Val Accuracy: 94.7%\n",
      "Epoch: 40/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1560... Val Accuracy: 95.7%\n",
      "Epoch: 41/50... Step: 0... Loss: 0.0000... Val Loss: 0.1368... Val Accuracy: 96.2%\n",
      "Epoch: 41/50... Step: 6000... Loss: 0.1828... Val Loss: 0.1246... Val Accuracy: 97.1%\n",
      "Epoch: 42/50... Step: 0... Loss: 0.0000... Val Loss: 0.2972... Val Accuracy: 94.3%\n",
      "Epoch: 42/50... Step: 6000... Loss: 0.1948... Val Loss: 0.2420... Val Accuracy: 95.7%\n",
      "Epoch: 43/50... Step: 0... Loss: 0.0218... Val Loss: 0.2154... Val Accuracy: 95.9%\n",
      "Epoch: 43/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2464... Val Accuracy: 96.1%\n",
      "Epoch: 44/50... Step: 0... Loss: 0.0001... Val Loss: 0.1351... Val Accuracy: 96.9%\n",
      "Epoch: 44/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2144... Val Accuracy: 96.7%\n",
      "Epoch: 45/50... Step: 0... Loss: 0.0017... Val Loss: 0.1929... Val Accuracy: 96.6%\n",
      "Epoch: 45/50... Step: 6000... Loss: 0.0022... Val Loss: 0.2600... Val Accuracy: 96.7%\n",
      "Epoch: 46/50... Step: 0... Loss: 0.0010... Val Loss: 0.9791... Val Accuracy: 94.1%\n",
      "Epoch: 46/50... Step: 6000... Loss: 0.0133... Val Loss: 0.2837... Val Accuracy: 97.2%\n",
      "Epoch: 47/50... Step: 0... Loss: 0.0000... Val Loss: 0.3825... Val Accuracy: 90.4%\n",
      "Epoch: 47/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2828... Val Accuracy: 96.9%\n",
      "Epoch: 48/50... Step: 0... Loss: 0.0068... Val Loss: 0.2316... Val Accuracy: 96.2%\n",
      "Epoch: 48/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2614... Val Accuracy: 96.6%\n",
      "Epoch: 49/50... Step: 0... Loss: 0.0514... Val Loss: 0.1893... Val Accuracy: 96.8%\n",
      "Epoch: 49/50... Step: 6000... Loss: 0.0000... Val Loss: 0.2263... Val Accuracy: 96.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50/50... Step: 0... Loss: 0.3716... Val Loss: 0.3838... Val Accuracy: 89.6%\n",
      "Epoch: 50/50... Step: 6000... Loss: 0.0000... Val Loss: 0.1620... Val Accuracy: 96.5%\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "batch_size=1\n",
    "lr=0.0001\n",
    "print_every=6000\n",
    "valid_batch = 1\n",
    "net.train()\n",
    "\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if(use_cuda):\n",
    "    net.cuda()\n",
    "all_valid_losses = [.6]\n",
    "counter = 0\n",
    "for e in range(epochs):\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        # print(batch_data[0][0].shape)\n",
    "        # inputs = batch_data[0][0].view(12,batch_size,3,32,32)\n",
    "        inputs = batch_data[0][0]\n",
    "        inputs = inputs[0:4]\n",
    "        # print(inputs.shape)\n",
    "        inputs = inputs.view(batch_size,12,32,32)\n",
    "        targets = batch_data[1]\n",
    "        if(use_cuda):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        # get the output from the model\n",
    "        output = net(inputs)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.view(batch_size,2), targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # loss stats\n",
    "        if batch_idx % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            correct = 0.\n",
    "            total = 0.\n",
    "            for val_batch_idx, batch_data in enumerate(valid_loader): \n",
    "                # inputs, targets = batch_data[0].view(12,valid_batch,1024), batch_data[1]\n",
    "                inputs, targets = batch_data[0][0], batch_data[1]\n",
    "                inputs = inputs[0:4]\n",
    "                inputs = inputs.view(batch_size,12,32,32)\n",
    "\n",
    "                if(use_cuda):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                output = net(inputs)\n",
    "                # print(output)\n",
    "                \n",
    "                val_loss = criterion(output.view(valid_batch,2), targets)\n",
    "                val_losses.append(val_loss.item())\n",
    "                pred = output[0].max(0,keepdim=True)[1]\n",
    "                # print(pred)\n",
    "                # sm = nn.Softmax(dim=1)\n",
    "                # sm_outputs = sm(output)\n",
    "                # print(sm_outputs)\n",
    "                # compare predictions to true label\n",
    "                correct += np.sum(np.squeeze(pred.eq(targets.data.view_as(pred))).cpu().numpy())\n",
    "                total += valid_batch\n",
    "\n",
    "            net.train() # reset to train mode after iteration through validation data\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(batch_idx),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "                  \"Val Accuracy: {:.1f}%\".format(correct/total*100))\n",
    "\n",
    "            if np.mean(val_losses) < np.min(all_valid_losses):\n",
    "                print(\"Validation loss dropped, saving model.\")\n",
    "                torch.save(net,'DNNModel_' + saveName + '.pt')\n",
    "                all_valid_losses = np.append(all_valid_losses,np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('DNNModel_' + saveName + '.pt')\n",
    "stateDictName ='DNN_StateDict_' + saveName + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), stateDictName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(stateDictName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct = 0.\n",
    "total = 0.\n",
    "false_pos = 0.\n",
    "false_neg = 0.\n",
    "test_batch = 1\n",
    "test_losses = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for test_batch_idx, batch_data in enumerate(test_loader):                    \n",
    "    inputs, targets = batch_data[0][0], batch_data[1]\n",
    "    inputs = inputs[0:4]\n",
    "    inputs = inputs.view(batch_size,12,32,32)\n",
    "\n",
    "    if(use_cuda):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    # print(inputs)\n",
    "    output = net(inputs)\n",
    "\n",
    "    test_loss = criterion(output.view(test_batch,2), targets)\n",
    "    test_losses.append(test_loss.item()/len(test_loader))\n",
    "\n",
    "    pred = output[0].max(0,keepdim=True)[1]\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct += np.sum(np.squeeze(pred.eq(targets.data.view_as(pred))).cpu().numpy())\n",
    "    \n",
    "    # number of false positives; hack for batch size = 1\n",
    "    false_pos += int(targets.data.cpu().numpy()[0] == 0 and pred.data.cpu().numpy()[0] == 1)\n",
    "    \n",
    "    # number of false negatives; hack for batch size = 1\n",
    "    false_neg += int(targets.data.cpu().numpy()[0] == 1 and pred.data.cpu().numpy()[0] == 0)\n",
    "    \n",
    "    total += test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104.0\n",
      "1141.0\n",
      "8.0\n",
      "29.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(correct)\n",
    "print(total)\n",
    "print(false_pos)\n",
    "print(false_neg)\n",
    "int(targets.data.cpu().numpy()[0] == 1 and pred.data.cpu().numpy()[0] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('DNNModel_' + saveName + '.pt')\n",
    "stateDictName ='DNN_StateDict_' + saveName + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), stateDictName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(stateDictName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('DNN_StateDict_S3_Iter_1.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip up state dict, get over to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below lines are only needed if allowing SageMaker to perform inference on the model.\n",
    "# I can walk you through that when you get there.\n",
    "\n",
    "#tarName = 'DNNModel_' + saveName + '.tar.gz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.system('tar -cvzf ' + tarName + ' ' + stateDictName)\n",
    "#os.system('aws s3 cp ' + tarName + ' s3://ypb-ml-images/' + tarName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_derm-ai)",
   "language": "python",
   "name": "conda_derm-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
